{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a22eb283294a02ad63a317ab7c9f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1611463294011_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-197.ec2.internal:20888/proxy/application_1611463294011_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-106.ec2.internal:8042/node/containerlogs/container_1611463294011_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Int = 4\n"
     ]
    }
   ],
   "source": [
    "2+2\n",
    "\n",
    "//hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "//hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "//hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1611463294011_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-197.ec2.internal:20888/proxy/application_1611463294011_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-106.ec2.internal:8042/node/containerlogs/container_1611463294011_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1611463294011_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-192-10-197.ec2.internal:20888/proxy/application_1611463294011_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-192-10-106.ec2.internal:8042/node/containerlogs/container_1611463294011_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3bd93bd7ef4f208a531314ba486f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "outputBucket: String = emr-demo-processed-250718677561-us-east-1\n",
      "inputDataBucket: String = emr-demo-raw-250718677561-us-east-1\n",
      "hudiTableName: String = indian_food_review_cow\n",
      "hudiTableRecordKey: String = foodid\n",
      "hudiTablePath: String = s3://emr-demo-processed-250718677561-us-east-1/createdataset/indian_food_review_cow\n",
      "hudiTablePartitionColumn: String = diet\n",
      "hudiTablePrecombineKey: String = timestamp\n",
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "root\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Initialize a Spark Session for Hudi\n",
    "// Copy on Write\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "\n",
    "\n",
    "//Where to Store Your Hudi Table \n",
    "val outputBucket = \"emr-demo-processed-250718677561-us-east-1\"\n",
    "// input data set \n",
    "val inputDataBucket = \"emr-demo-raw-250718677561-us-east-1\"\n",
    "\n",
    "//Specify common DataSourceWriteOptions int a single hudiOption variable \n",
    "val hudiTableName = \"indian_food_review_cow\"\n",
    "val hudiTableRecordKey = \"foodid\"\n",
    "val hudiTablePath = \"s3://\"+outputBucket+\"/createdataset/\"+hudiTableName\n",
    "val hudiTablePartitionColumn = \"diet\"\n",
    "val hudiTablePrecombineKey = \"timestamp\"\n",
    "\n",
    "\n",
    "// read data from s3 \n",
    "val sourceData = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/indian_food_withID_full.csv\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "\n",
    "sourceData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cf990f0f7d48acab31a6714b2d3bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|foodid|          name|      diet|        state|flavor_profile| course|        state| timestamp|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|     1|    Balu shahi|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|     2|        Boondi|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1611463753|\n",
      "|     3|Gajar ka halwa|vegetarian|       Punjab|         sweet|dessert|       Punjab|1611463753|\n",
      "|     4|        Ghevar|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1611463753|\n",
      "|     5|   Gulab jamun|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|     6|        Imarti|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|     7|        Jalebi|vegetarian|Uttar Pradesh|         sweet|dessert|Uttar Pradesh|1611463753|\n",
      "|     8|    Kaju katli|vegetarian|           -1|         sweet|dessert|           -1|1611463753|\n",
      "|     9|      Kalakand|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|    10|         Kheer|vegetarian|           -1|         sweet|dessert|           -1|1611463753|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sourceData.select(\"foodid\",\"name\",\"diet\",\"state\",\"flavor_profile\",\"course\",\"state\",\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d50fd88a3c74f7da662f8b5dd6a49f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> diet, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> indian_food_review_cow, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> foodid, hoodie.table.name -> indian_food_review_cow, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.write.partitionpath.field -> diet)\n"
     ]
    }
   ],
   "source": [
    "// Set up our Hudi Data Source Options\n",
    "val hudiOptions = Map[String,String](\n",
    "    HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "    //for this detaset use COPY_ON_WRITE storage strategy other option us MERGE_ON_READ\n",
    "    DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "    //next three options configure what Hudi should use as its record key \n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"foodid\",\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "    //for this data set, we specify that we want to sync metadata with Hive \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "    DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> classOf[MultiPartKeysValueExtractor].getName\n",
    "    )\n",
    "\n",
    "//getName,\n",
    "//    \"hoodie.parquet.max.file.size\" -> String.valueOf(1024 * 1024 * 1024),\n",
    "//    \"hoodie.parquet.small.file.limit\" -> String.valueOf(64 * 1024 * 1024),\n",
    "//    \"hoodie.parquet.compression.ratio\" -> String.valueOf(0.5),\n",
    "//    \"hoodie.insert.shuffle.parallelism\" -> String.valueOf(2))\n",
    "\n",
    "\n",
    "\n",
    "// Write input data to Hudi \n",
    "\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    " // Opreation  Key tells Hudi whether this is an Insert Upsert or Bulk Insert operation \n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed23a079e5ce489b87bb423cfc5432f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read Hudi data set from s3 \n",
    "\n",
    "val readOptimzedHudiViewDF = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "\n",
    "// take a look at our data ... \n",
    "readOptimzedHudiViewDF.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b715b882704790b6087cbefa3487f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|foodid|state|\n",
      "+------+-----+\n",
      "|     8|   -1|\n",
      "|    10|   -1|\n",
      "|    11|   -1|\n",
      "|    13|   -1|\n",
      "|    95|   -1|\n",
      "|    97|   -1|\n",
      "|    99|   -1|\n",
      "|   110|   -1|\n",
      "|   112|   -1|\n",
      "|   116|   -1|\n",
      "|   118|   -1|\n",
      "|   129|   -1|\n",
      "|   131|   -1|\n",
      "|   145|   -1|\n",
      "|   146|   -1|\n",
      "|   150|   -1|\n",
      "|   155|   -1|\n",
      "|   157|   -1|\n",
      "|   159|   -1|\n",
      "|   162|   -1|\n",
      "|   163|   -1|\n",
      "|   165|   -1|\n",
      "|   232|   -1|\n",
      "|   249|   -1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select foodid , state  from indian_food_ro_table where state='-1' limit 25 \"\"\" ).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e35dd0ea0b4dcb8af71aea187c4cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readOptimzedHudiViewDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f97cc244afd41b7b23dd9904ff61804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|foodid|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+\n",
      "|     20210124044941|  20210124044941_1_1|                 1|            vegetarian|e5a3eba9-336f-4a4...|     1|\n",
      "|     20210124044941|  20210124044941_1_2|                 2|            vegetarian|e5a3eba9-336f-4a4...|     2|\n",
      "|     20210124044941|  20210124044941_1_3|                 3|            vegetarian|e5a3eba9-336f-4a4...|     3|\n",
      "|     20210124044941|  20210124044941_1_4|                 4|            vegetarian|e5a3eba9-336f-4a4...|     4|\n",
      "|     20210124044941|  20210124044941_1_5|                 5|            vegetarian|e5a3eba9-336f-4a4...|     5|\n",
      "|     20210124044941|  20210124044941_1_6|                 6|            vegetarian|e5a3eba9-336f-4a4...|     6|\n",
      "|     20210124044941|  20210124044941_1_7|                 7|            vegetarian|e5a3eba9-336f-4a4...|     7|\n",
      "|     20210124044941|  20210124044941_1_8|                 8|            vegetarian|e5a3eba9-336f-4a4...|     8|\n",
      "|     20210124044941|  20210124044941_1_9|                 9|            vegetarian|e5a3eba9-336f-4a4...|     9|\n",
      "|     20210124044941| 20210124044941_1_10|                10|            vegetarian|e5a3eba9-336f-4a4...|    10|\n",
      "|     20210124044941| 20210124044941_1_11|                11|            vegetarian|e5a3eba9-336f-4a4...|    11|\n",
      "|     20210124044941| 20210124044941_1_12|                12|            vegetarian|e5a3eba9-336f-4a4...|    12|\n",
      "|     20210124044941| 20210124044941_1_13|                13|            vegetarian|e5a3eba9-336f-4a4...|    13|\n",
      "|     20210124044941| 20210124044941_1_14|                14|            vegetarian|e5a3eba9-336f-4a4...|    14|\n",
      "|     20210124044941| 20210124044941_1_15|                15|            vegetarian|e5a3eba9-336f-4a4...|    15|\n",
      "|     20210124044941| 20210124044941_1_16|                16|            vegetarian|e5a3eba9-336f-4a4...|    16|\n",
      "|     20210124044941| 20210124044941_1_17|                17|            vegetarian|e5a3eba9-336f-4a4...|    17|\n",
      "|     20210124044941| 20210124044941_1_18|                18|            vegetarian|e5a3eba9-336f-4a4...|    18|\n",
      "|     20210124044941| 20210124044941_1_19|                19|            vegetarian|e5a3eba9-336f-4a4...|    19|\n",
      "|     20210124044941| 20210124044941_1_20|                20|            vegetarian|e5a3eba9-336f-4a4...|    20|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select _hoodie_commit_time ,_hoodie_commit_seqno ,_hoodie_record_key , _hoodie_partition_path , _hoodie_file_name , foodid from indian_food_ro_table limit 20 \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbba6b162ac243a0a958c275a50367d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.net.URI\n",
      "import org.apache.hadoop.fs.FileSystem\n",
      "import org.apache.hadoop.fs.Path\n",
      "res40: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "// remove files from s3 raw bucket \n",
    "\n",
    "import java.net.URI\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "FileSystem.get(new URI(\"s3n://\"+inputDataBucket), sc.hadoopConfiguration).delete(new Path(\"s3n://\"+inputDataBucket+\"/\"), true)\n",
    "\n",
    "//get Delta file - Nifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2bce87eaa34b498ced4bc9dc226fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData_delta: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "root\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read data from s3 - Delta data \n",
    "val sourceData_delta = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "\n",
    "sourceData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22774c7e6c1e47a0aa697da059e7944b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+------+--------+\n",
      "| state|count(1)|\n",
      "+------+--------+\n",
      "|Punjab|      24|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// take a look at our data ... \n",
    "sourceData_delta.registerTempTable(\"indian_food_delta\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_delta group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cdb81ba02b4ab487dbeda107fba7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+----------+---------+---------+--------------+-----------+------+------+----------+\n",
      "|foodid|          name|         ingredients|      diet|prep_time|cook_time|flavor_profile|     course| state|region| timestamp|\n",
      "+------+--------------+--------------------+----------+---------+---------+--------------+-----------+------+------+----------+\n",
      "|     8|    Kaju katli|Cashews, ghee, ca...|vegetarian|       10|       20|         sweet|    dessert|Punjab|    -1|1611463905|\n",
      "|    10|         Kheer|Milk, rice, sugar...|vegetarian|       10|       40|         sweet|    dessert|Punjab|    -1|1611463905|\n",
      "|    11|         Laddu|Gram flour, ghee,...|vegetarian|       10|       40|         sweet|    dessert|Punjab|    -1|1611463905|\n",
      "|    13|     Nankhatai|Refined flour, be...|vegetarian|       20|       30|         sweet|    dessert|Punjab|    -1|1611463905|\n",
      "|    95|       Khichdi|Moong dal, green ...|vegetarian|       40|       20|         spicy|main course|Punjab|    -1|1611463905|\n",
      "|    97| Kulfi falooda|Rose syrup, faloo...|vegetarian|       45|       25|         sweet|    dessert|Punjab|    -1|1611463905|\n",
      "|    99|Lauki ki subji|Bottle gourd, coc...|vegetarian|       10|       20|         spicy|main course|Punjab|    -1|1611463905|\n",
      "|   110|     Pani puri|Kala chana, mashe...|vegetarian|       15|        2|         spicy|      snack|Punjab|    -1|1611463905|\n",
      "|   112|         Papad|Urad dal, sev, le...|vegetarian|        5|        5|         spicy|      snack|Punjab|    -1|1611463905|\n",
      "|   116|  Rajma chaval|Red kidney beans,...|vegetarian|       15|       90|         spicy|main course|Punjab| North|1611463905|\n",
      "|   118|        Samosa|Potatoes, green p...|vegetarian|       30|       30|         spicy|      snack|Punjab|    -1|1611463905|\n",
      "|   129|          Dosa|Chana dal, urad d...|vegetarian|      360|       90|         spicy|      snack|Punjab| South|1611463905|\n",
      "|   131|          Idli|Split urad dal, u...|vegetarian|      360|       90|         spicy|      snack|Punjab| South|1611463905|\n",
      "|   146|       Pachadi|Coconut oil, cucu...|vegetarian|       10|       25|            -1|main course|Punjab| South|1611463905|\n",
      "|   150|       Payasam|Rice, cashew nuts...|vegetarian|       15|       30|         sweet|    dessert|Punjab| South|1611463905|\n",
      "|   155|         Rasam|Tomato, curry lea...|vegetarian|       10|       35|         spicy|main course|Punjab| South|1611463905|\n",
      "|   157|        Sambar|Pigeon peas, eggp...|vegetarian|       20|       45|         spicy|main course|Punjab| South|1611463905|\n",
      "|   159|         Sevai|Sevai, parboiled ...|vegetarian|      120|       30|            -1|main course|Punjab| South|1611463905|\n",
      "|   162|       Uttapam|Chana dal, urad d...|vegetarian|       10|       20|         spicy|      snack|Punjab| South|1611463905|\n",
      "|   163|          Vada|Urad dal, ginger,...|vegetarian|       15|       20|         spicy|      snack|Punjab| South|1611463905|\n",
      "|   165|          Upma|Chana dal, urad d...|vegetarian|       10|       20|         spicy|      snack|Punjab|    -1|1611463905|\n",
      "|   232|    Brown Rice|Brown rice, soy s...|vegetarian|       15|       25|            -1|main course|Punjab|    -1|1611463905|\n",
      "|   249|      Red Rice|Red pepper, red o...|vegetarian|       -1|       -1|            -1|main course|Punjab|    -1|1611463905|\n",
      "|   145|   Masala Dosa|Chana dal, urad d...|vegetarian|      360|       90|         spicy|      snack|Punjab| South|1611463905|\n",
      "+------+--------------+--------------------+----------+---------+---------+--------------+-----------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from indian_food_delta \"\"\" ).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c520b615d381439ca7748863f8329f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Update \n",
    "\n",
    "// IMP - Before , if you wanted to update data in s3 ,, you have to read the old data , merge with the new data\n",
    "// and then overwrite the old data .. with Hudi , we can directly update the data in-Place\n",
    "\n",
    "(sourceData_delta.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .options(hudiOptions)\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c289415e81fc4e75801c15c053004fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_updated: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      56|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "|      Telangana|       5|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_updated = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_updated.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de32a551c1f4fc18eb4b134507c2676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.net.URI\n",
      "import org.apache.hadoop.fs.FileSystem\n",
      "import org.apache.hadoop.fs.Path\n",
      "res70: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "// remove files from s3 raw bucket \n",
    "\n",
    "import java.net.URI\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "FileSystem.get(new URI(\"s3n://\"+inputDataBucket), sc.hadoopConfiguration).delete(new Path(\"s3n://\"+inputDataBucket+\"/\"), true)\n",
    "\n",
    "//get Delta file for delete ops - Nifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af4050a244a4cf3b3b5d1856f147c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData_delta_delete: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "root\n",
      " |-- foodid: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- diet: string (nullable = true)\n",
      " |-- prep_time: string (nullable = true)\n",
      " |-- cook_time: string (nullable = true)\n",
      " |-- flavor_profile: string (nullable = true)\n",
      " |-- course: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read data from s3 - Delta data \n",
    "val sourceData_delta_delete = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "\n",
    "sourceData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c3963a8c7244a3a187cafafe4e251c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "|foodid|    name|         ingredients|          diet|prep_time|cook_time|flavor_profile|     course|state|region| timestamp|\n",
      "+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "|   212|Vindaloo|Chicken, coconut ...|non_vegetarian|       10|       40|         spicy|main course|  Goa|  West|1611464602|\n",
      "|   252| Bebinca|Coconut milk, egg...|    vegetarian|       20|       60|         sweet|    dessert|  Goa|  West|1611464602|\n",
      "|   255|  Pinaca|Brown rice, fenne...|    vegetarian|       -1|       -1|         sweet|    dessert|  Goa|  West|1611464602|\n",
      "+------+--------+--------------------+--------------+---------+---------+--------------+-----------+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// take a look at our data ... \n",
    "sourceData_delta_delete.registerTempTable(\"indian_food_delta\");\n",
    "spark.sql(\"\"\"select * from indian_food_delta  \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5b6a2322bc4763af2fe90f0f07e541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(sourceData_delta_delete.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .options(hudiOptions)\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\")\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath))\n",
    "\n",
    "\n",
    "//You can also hard delete data by setting OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL to\n",
    "//remove all records in the dataset you submit. For instructions on performing soft deletes, \n",
    "//and for more information about deleting data stored in Hudi tables, see Deletes in the Apache Hudi documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f693dc6e23d4f48838de00610a01758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_postdelete: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      56|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "|      Telangana|       5|\n",
      "|        Tripura|       1|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_postdelete = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_postdelete.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96933a32ada4ef0a04091cabbc8f509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: Array[String] = Array(20210124044941, 20210124045902)\n",
      "res95: String = [Ljava.lang.String;@4a5bf4c4\n"
     ]
    }
   ],
   "source": [
    "// Point in time SQL \n",
    "\n",
    "\n",
    "val commits = (spark.sql(\"\"\" select distinct(_hoodie_commit_time) as commitTime from \n",
    "indian_food_ro_table order by commitTime\"\"\").map(k=> k.getString(0)).take(50))\n",
    "\n",
    "commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af60244e039d49fe92584ca3374b8f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.hudi.DataSourceReadOptions\n",
      "beginTime: String = 0\n",
      "endTime: String = 20210124044941\n",
      "dataSet: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.hudi.DataSourceReadOptions\n",
    "val beginTime = \"0\"\n",
    "\n",
    "val endTime = commits(0)\n",
    "\n",
    "val dataSet = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n",
    "\n",
    "dataSet.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6785633cb3493cb10b0d5621bcf823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 20210124044941\n",
      "endTime: String = 20210124045902\n",
      "dataSet1: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+------+--------+\n",
      "| state|count(1)|\n",
      "+------+--------+\n",
      "|Punjab|      24|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val beginTime = commits(0)\n",
    "\n",
    "val endTime = commits(1)\n",
    "\n",
    "val dataSet1 = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n",
    "\n",
    "dataSet1.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0328001c8dde4234b346ea90785d33d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 20210124045902\n",
      "dataSet2: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+-----+--------+\n",
      "|state|count(1)|\n",
      "+-----+--------+\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val beginTime = commits(1)\n",
    "\n",
    "//val endTime = commits(1)\n",
    "\n",
    "val dataSet2 = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     // Mark that we want to do an incremental query \n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime) \n",
    "     //.option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)  \n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)) \n",
    "\n",
    "dataSet2.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b57c7608ed459dbc21eb02e3eae452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.net.URI\n",
      "import org.apache.hadoop.fs.FileSystem\n",
      "import org.apache.hadoop.fs.Path\n",
      "res127: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "// remove files from s3 raw bucket \n",
    "\n",
    "import java.net.URI\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "FileSystem.get(new URI(\"s3n://\"+inputDataBucket), sc.hadoopConfiguration).delete(new Path(\"s3n://\"+inputDataBucket+\"/\"), true)\n",
    "\n",
    "//get full file for  - Nifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9349244d75e54796b508a0dbb86c8661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputBucket: String = emr-demo-processed-250718677561-us-east-1\n",
      "inputDataBucket: String = emr-demo-raw-250718677561-us-east-1\n",
      "hudiTableName: String = indian_food_review_mor\n",
      "hudiTableRecordKey: String = foodid\n",
      "hudiTablePath: String = s3://emr-demo-processed-250718677561-us-east-1/createdataset/indian_food_review_mor\n",
      "hudiTablePartitionColumn: String = diet\n",
      "hudiTablePrecombineKey: String = timestamp\n",
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> diet, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> indian_food_review_mor, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> foodid, hoodie.table.name -> indian_food_review_mor, hoodie.datasource.write.storage.type -> MERGE_ON_READ, hoodie.datasource.write.partitionpath.field -> diet)\n",
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|foodid|          name|      diet|        state|flavor_profile| course|        state| timestamp|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "|     1|    Balu shahi|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|     2|        Boondi|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1611463753|\n",
      "|     3|Gajar ka halwa|vegetarian|       Punjab|         sweet|dessert|       Punjab|1611463753|\n",
      "|     4|        Ghevar|vegetarian|    Rajasthan|         sweet|dessert|    Rajasthan|1611463753|\n",
      "|     5|   Gulab jamun|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|     6|        Imarti|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|     7|        Jalebi|vegetarian|Uttar Pradesh|         sweet|dessert|Uttar Pradesh|1611463753|\n",
      "|     8|    Kaju katli|vegetarian|           -1|         sweet|dessert|           -1|1611463753|\n",
      "|     9|      Kalakand|vegetarian|  West Bengal|         sweet|dessert|  West Bengal|1611463753|\n",
      "|    10|         Kheer|vegetarian|           -1|         sweet|dessert|           -1|1611463753|\n",
      "+------+--------------+----------+-------------+--------------+-------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Using MoR - Hudi\n",
    "\n",
    "//Where to Store Your Hudi Table \n",
    "val outputBucket = \"emr-demo-processed-250718677561-us-east-1\"\n",
    "// input data set \n",
    "val inputDataBucket = \"emr-demo-raw-250718677561-us-east-1\"\n",
    "\n",
    "val hudiTableName = \"indian_food_review_mor\"\n",
    "val hudiTableRecordKey = \"foodid\"\n",
    "val hudiTablePath = \"s3://\"+outputBucket+\"/createdataset/\"+hudiTableName\n",
    "val hudiTablePartitionColumn = \"diet\"\n",
    "val hudiTablePrecombineKey = \"timestamp\"\n",
    "\n",
    "\n",
    "val hudiOptions = Map[String,String](\n",
    "    HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "    //for this detaset use COPY_ON_WRITE storage strategy other option us MERGE_ON_READ\n",
    "    DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"MERGE_ON_READ\", \n",
    "    //next three options configure what Hudi should use as its record key \n",
    "    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"foodid\",\n",
    "    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "    //for this data set, we specify that we want to sync metadata with Hive \n",
    "    DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "    DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"diet\",\n",
    "    DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> classOf[MultiPartKeysValueExtractor].getName\n",
    "    )\n",
    "\n",
    "// read data from s3 \n",
    "val sourceData = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/*\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "\n",
    "//sourceData.printSchema()\n",
    "\n",
    "sourceData.select(\"foodid\",\"name\",\"diet\",\"state\",\"flavor_profile\",\"course\",\"state\",\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d02cec5a8f44e2a8d5963c8370bc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Write input data to Hudi \n",
    "\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    " // Opreation  Key tells Hudi whether this is an Insert Upsert or Bulk Insert operation \n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n",
    "  .options(hudiOptions)\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b11ea73de247f7b8b17828a664ac6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[default,indian_food_review_cow,false]\n",
      "[default,indian_food_review_mor_ro,false]\n",
      "[default,indian_food_review_mor_rt,false]\n",
      "[,indian_food_delta,true]\n",
      "[,indian_food_ro_table,true]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500bbc32fb8143dba321791e67cb62f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+--------------------+---------+---------+--------------+-------+-----------+------+----------+----------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|foodid|          name|         ingredients|prep_time|cook_time|flavor_profile| course|      state|region| timestamp|      diet|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+--------------------+---------+---------+--------------+-------+-----------+------+----------+----------+\n",
      "|     20210124051536| 20210124051536_1_54|                 1|            vegetarian|4ce369f6-33b0-41c...|     1|    Balu shahi|Maida flour, yogu...|       45|       25|         sweet|dessert|West Bengal|  East|1611463753|vegetarian|\n",
      "|     20210124051536| 20210124051536_1_55|                 2|            vegetarian|4ce369f6-33b0-41c...|     2|        Boondi|Gram flour, ghee,...|       80|       30|         sweet|dessert|  Rajasthan|  West|1611463753|vegetarian|\n",
      "|     20210124051536| 20210124051536_1_56|                 3|            vegetarian|4ce369f6-33b0-41c...|     3|Gajar ka halwa|Carrots, milk, su...|       15|       60|         sweet|dessert|     Punjab| North|1611463753|vegetarian|\n",
      "|     20210124051536| 20210124051536_1_57|                 4|            vegetarian|4ce369f6-33b0-41c...|     4|        Ghevar|Flour, ghee, kewr...|       15|       30|         sweet|dessert|  Rajasthan|  West|1611463753|vegetarian|\n",
      "|     20210124051536| 20210124051536_1_58|                 5|            vegetarian|4ce369f6-33b0-41c...|     5|   Gulab jamun|Milk powder, plai...|       15|       40|         sweet|dessert|West Bengal|  East|1611463753|vegetarian|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+--------------+--------------------+---------+---------+--------------+-------+-----------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from indian_food_review_mor_ro\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a742293c4f47d1b3ba4c458e687dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select state , count(*) from indian_food_review_mor_ro group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa9577232c540e89a1565b4ad03b60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select state , count(*) from indian_food_review_mor_rt group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf764ea0e3046c5bd7d6300a4838f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "|             -1|      24|\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      32|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read Hudi data set from s3 \n",
    "\n",
    "val readOptimzedHudiViewDF = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "// take a look at our data ... \n",
    "readOptimzedHudiViewDF.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdf25599ce34131ac362a3c312d8cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.net.URI\n",
      "import org.apache.hadoop.fs.FileSystem\n",
      "import org.apache.hadoop.fs.Path\n",
      "res173: Boolean = true\n"
     ]
    }
   ],
   "source": [
    "// remove files from s3 raw bucket \n",
    "\n",
    "import java.net.URI\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "FileSystem.get(new URI(\"s3n://\"+inputDataBucket), sc.hadoopConfiguration).delete(new Path(\"s3n://\"+inputDataBucket+\"/\"), true)\n",
    "\n",
    "//get full file for  - Nifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22c89c59d2645e59dcb563c2759bb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData_delta: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [foodid: string, name: string ... 9 more fields]\n",
      "+------+--------------+----------+------+--------------+-----------+------+----------+\n",
      "|foodid|          name|      diet| state|flavor_profile|     course| state| timestamp|\n",
      "+------+--------------+----------+------+--------------+-----------+------+----------+\n",
      "|     8|    Kaju katli|vegetarian|Punjab|         sweet|    dessert|Punjab|1611465599|\n",
      "|    10|         Kheer|vegetarian|Punjab|         sweet|    dessert|Punjab|1611465599|\n",
      "|    11|         Laddu|vegetarian|Punjab|         sweet|    dessert|Punjab|1611465599|\n",
      "|    13|     Nankhatai|vegetarian|Punjab|         sweet|    dessert|Punjab|1611465599|\n",
      "|    95|       Khichdi|vegetarian|Punjab|         spicy|main course|Punjab|1611465599|\n",
      "|    97| Kulfi falooda|vegetarian|Punjab|         sweet|    dessert|Punjab|1611465599|\n",
      "|    99|Lauki ki subji|vegetarian|Punjab|         spicy|main course|Punjab|1611465599|\n",
      "|   110|     Pani puri|vegetarian|Punjab|         spicy|      snack|Punjab|1611465599|\n",
      "|   112|         Papad|vegetarian|Punjab|         spicy|      snack|Punjab|1611465599|\n",
      "|   116|  Rajma chaval|vegetarian|Punjab|         spicy|main course|Punjab|1611465599|\n",
      "+------+--------------+----------+------+--------------+-----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read data from s3 \n",
    "val sourceData_delta = (spark.read.option(\"header\",true).csv(\"s3://\"+inputDataBucket+\"/*\")\n",
    "             .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                 .cache())\n",
    "\n",
    "//sourceData.printSchema()\n",
    "\n",
    "sourceData_delta.select(\"foodid\",\"name\",\"diet\",\"state\",\"flavor_profile\",\"course\",\"state\",\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5612cac8b1e14dddba90c0fc1adb71a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(sourceData_delta.write\n",
    " .format(\"org.apache.hudi\")\n",
    " .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    " .options(hudiOptions)\n",
    " .mode(SaveMode.Append)\n",
    " .save(hudiTablePath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac99d55609904a9db625dc375816a553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimzedHudiViewDF_updated: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 14 more fields]\n",
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      56|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "|      Telangana|       5|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// read OptimizedHudiViewDF from updated data set \n",
    "\n",
    "val readOptimzedHudiViewDF_updated = (spark.read\n",
    "                             .format(\"org.apache.hudi\")\n",
    "                             .load(hudiTablePath+ \"/*\"))\n",
    "\n",
    "readOptimzedHudiViewDF_updated.registerTempTable(\"indian_food_ro_table\");\n",
    "spark.sql(\"\"\"select state , count(*) from indian_food_ro_table group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002e1a614e344f4cb315b4b6b5d44158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|          state|count(1)|\n",
      "+---------------+--------+\n",
      "| Andhra Pradesh|      10|\n",
      "|          Assam|      21|\n",
      "|          Bihar|       3|\n",
      "|   Chhattisgarh|       1|\n",
      "|            Goa|       3|\n",
      "|        Gujarat|      35|\n",
      "|        Haryana|       1|\n",
      "|Jammu & Kashmir|       2|\n",
      "|      Karnataka|       6|\n",
      "|         Kerala|       8|\n",
      "| Madhya Pradesh|       2|\n",
      "|    Maharashtra|      30|\n",
      "|        Manipur|       2|\n",
      "|   NCT of Delhi|       1|\n",
      "|       Nagaland|       1|\n",
      "|         Odisha|       7|\n",
      "|         Punjab|      56|\n",
      "|      Rajasthan|       6|\n",
      "|     Tamil Nadu|      20|\n",
      "|      Telangana|       5|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select state , count(*) from indian_food_review_mor_ro group by \n",
    "state order by state ASC \"\"\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
